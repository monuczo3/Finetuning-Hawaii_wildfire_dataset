{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0ljqH6SOuUT8TOVtbn4Jn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monuczo3/Finetuning-Hawaii_wildfire_dataset/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPNZDFYKv_eO"
      },
      "outputs": [],
      "source": [
        "!pip install peft\n",
        "!pip install accelerate\n",
        "!pip install bitsandBytes\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install GPUtil"
      ],
      "metadata": {
        "id": "DQJ6ApTbwEi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import GPUtil\n",
        "import os\n",
        "\n",
        "GPUtil.showUtilization()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available\")\n",
        "else:\n",
        "  print(\"GPU is not available, using CPU instead\")\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "jxZ0mXQ3wFyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer\n",
        "from huggingface_hub import notebook_login\n",
        "from datasets import load_dataset\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        ""
      ],
      "metadata": {
        "id": "uLBvFhiOwIG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"COLAB_GPU\" in os.environ:\n",
        "  !huggingface-cli login\n",
        "else:\n",
        "  notebook_login()"
      ],
      "metadata": {
        "id": "zKXZ7M6yzmxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config = bnb_config)"
      ],
      "metadata": {
        "id": "sgDA-v8C0CJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/poloclub/Fine-tuning-LLMs.git"
      ],
      "metadata": {
        "id": "-YDwTdHO5TRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"text\",data_files={\"train\":\n",
        "                                                [\n",
        "                                                \"/content/Fine-tuning-LLMs/data/hawaii_wf_2.txt\",\n",
        "                                                \"/content/Fine-tuning-LLMs/data/hawaii_wf_4.txt\"]},\n",
        "                             split=\"train\")"
      ],
      "metadata": {
        "id": "O26I47eo6mi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[\"text\"][10]"
      ],
      "metadata": {
        "id": "NicQf2PPBL0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast= False, trust_remote_code=True,\n",
        "                                           add_eos_token=True)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.add_special_tokens({\"pad_token\":tokenizer.eos_token})"
      ],
      "metadata": {
        "id": "ylmLi6utBVDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset = []\n",
        "for phrase in train_dataset:\n",
        "  tokenized_train_dataset.append(tokenizer(phrase[\"text\"]))"
      ],
      "metadata": {
        "id": "QI1ejJ1VDPb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset[1]"
      ],
      "metadata": {
        "id": "VHTSAmkJDjEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha = 64,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    bias =\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model,config)"
      ],
      "metadata": {
        "id": "a5USbCu4Dlx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model = model,\n",
        "    train_dataset = tokenized_train_dataset,\n",
        "    args = transformers.TrainingArguments(\n",
        "        output_dir = \"./finetunedModel\",\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs = 3,\n",
        "        learning_rate=1e-4,\n",
        "        max_steps = 20,\n",
        "        bf16=False,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_dir=\"./log\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_steps=50,\n",
        "        logging_steps=10\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer,mlm=False)\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Aey2gBy5JrGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig, LlamaTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "n4fConfig = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    quantization_config = n4fConfig,\n",
        "    device_map = \"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        ")"
      ],
      "metadata": {
        "id": "E7-6ml8ZOPgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(base_model_id, use_fast=False, trust_remote_code=True, add_eos_token=True)\n",
        "\n",
        "modelFinetuned = PeftModel.from_pretrained(base_model,\"finetunedModel/checkpoint-20\")"
      ],
      "metadata": {
        "id": "4cQetqHSRL69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"When did Hawaii wildfires start?\"\n",
        "\n",
        "eval_prompt = f\"Question: {user_question} Just answer this question accurately and concisely\"\n",
        "\n",
        "promptTokenized = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "modelFinetuned.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  print(tokenizer.decode(modelFinetuned.generate(**promptTokenized, max_new_tokens=1024)[0], skip_special_tokens=True))\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "i6h-pHKHUsvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pv8Fo8ZCUwG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}